diff --git a/A2SP_out/.allenact_last_start_time_string b/A2SP_out/.allenact_last_start_time_string
deleted file mode 100644
index e6b148bf..00000000
--- a/A2SP_out/.allenact_last_start_time_string
+++ /dev/null
@@ -1 +0,0 @@
-2023-02-01_07-29-59
\ No newline at end of file
diff --git a/A2SP_out/.allenact_start_time_string.lock b/A2SP_out/.allenact_start_time_string.lock
deleted file mode 100755
index e69de29b..00000000
diff --git a/A2SP_out/tb/SymbolicCoordinatePPO/2023-02-01_07-28-00/events.out.tfevents.1675236480.sirius__train_2023-02-01_07-28-00 b/A2SP_out/tb/SymbolicCoordinatePPO/2023-02-01_07-28-00/events.out.tfevents.1675236480.sirius__train_2023-02-01_07-28-00
deleted file mode 100644
index e99b7ef0..00000000
Binary files a/A2SP_out/tb/SymbolicCoordinatePPO/2023-02-01_07-28-00/events.out.tfevents.1675236480.sirius__train_2023-02-01_07-28-00 and /dev/null differ
diff --git a/A2SP_out/tb/SymbolicCoordinatePPO/2023-02-01_07-29-59/events.out.tfevents.1675236599.sirius__train_2023-02-01_07-29-59 b/A2SP_out/tb/SymbolicCoordinatePPO/2023-02-01_07-29-59/events.out.tfevents.1675236599.sirius__train_2023-02-01_07-29-59
deleted file mode 100644
index 93abf379..00000000
Binary files a/A2SP_out/tb/SymbolicCoordinatePPO/2023-02-01_07-29-59/events.out.tfevents.1675236599.sirius__train_2023-02-01_07-29-59 and /dev/null differ
diff --git a/A2SP_out/used_configs/SymbolicCoordinatePPO/2023-02-01_07-28-00/3d4aade3.patch b/A2SP_out/used_configs/SymbolicCoordinatePPO/2023-02-01_07-28-00/3d4aade3.patch
deleted file mode 100644
index 3ad4a904..00000000
--- a/A2SP_out/used_configs/SymbolicCoordinatePPO/2023-02-01_07-28-00/3d4aade3.patch
+++ /dev/null
@@ -1,6 +0,0 @@
-diff --git a/projects/ithor_A2SP b/projects/ithor_A2SP
---- a/projects/ithor_A2SP
-+++ b/projects/ithor_A2SP
-@@ -1 +1 @@
--Subproject commit c234a1827742cca4ad591bb19c0282b566a54122
-+Subproject commit c234a1827742cca4ad591bb19c0282b566a54122-dirty
diff --git a/A2SP_out/used_configs/SymbolicCoordinatePPO/2023-02-01_07-28-00/abc.py b/A2SP_out/used_configs/SymbolicCoordinatePPO/2023-02-01_07-28-00/abc.py
deleted file mode 100644
index cfabe90b..00000000
--- a/A2SP_out/used_configs/SymbolicCoordinatePPO/2023-02-01_07-28-00/abc.py
+++ /dev/null
@@ -1,131 +0,0 @@
-### THIS FILE ORIGINALLY LOCATED AT '/home/zidong/anaconda3/envs/A2SP_env/lib/python3.8/abc.py'
-
-# Copyright 2007 Google, Inc. All Rights Reserved.
-# Licensed to PSF under a Contributor Agreement.
-
-"""Abstract Base Classes (ABCs) according to PEP 3119."""
-
-
-def abstractmethod(funcobj):
-    """A decorator indicating abstract methods.
-
-    Requires that the metaclass is ABCMeta or derived from it.  A
-    class that has a metaclass derived from ABCMeta cannot be
-    instantiated unless all of its abstract methods are overridden.
-    The abstract methods can be called using any of the normal
-    'super' call mechanisms.  abstractmethod() may be used to declare
-    abstract methods for properties and descriptors.
-
-    Usage:
-
-        class C(metaclass=ABCMeta):
-            @abstractmethod
-            def my_abstract_method(self, ...):
-                ...
-    """
-    funcobj.__isabstractmethod__ = True
-    return funcobj
-
-
-class abstractclassmethod(classmethod):
-    """A decorator indicating abstract classmethods.
-
-    Deprecated, use 'classmethod' with 'abstractmethod' instead.
-    """
-
-    __isabstractmethod__ = True
-
-    def __init__(self, callable):
-        callable.__isabstractmethod__ = True
-        super().__init__(callable)
-
-
-class abstractstaticmethod(staticmethod):
-    """A decorator indicating abstract staticmethods.
-
-    Deprecated, use 'staticmethod' with 'abstractmethod' instead.
-    """
-
-    __isabstractmethod__ = True
-
-    def __init__(self, callable):
-        callable.__isabstractmethod__ = True
-        super().__init__(callable)
-
-
-class abstractproperty(property):
-    """A decorator indicating abstract properties.
-
-    Deprecated, use 'property' with 'abstractmethod' instead.
-    """
-
-    __isabstractmethod__ = True
-
-
-try:
-    from _abc import (get_cache_token, _abc_init, _abc_register,
-                      _abc_instancecheck, _abc_subclasscheck, _get_dump,
-                      _reset_registry, _reset_caches)
-except ImportError:
-    from _py_abc import ABCMeta, get_cache_token
-    ABCMeta.__module__ = 'abc'
-else:
-    class ABCMeta(type):
-        """Metaclass for defining Abstract Base Classes (ABCs).
-
-        Use this metaclass to create an ABC.  An ABC can be subclassed
-        directly, and then acts as a mix-in class.  You can also register
-        unrelated concrete classes (even built-in classes) and unrelated
-        ABCs as 'virtual subclasses' -- these and their descendants will
-        be considered subclasses of the registering ABC by the built-in
-        issubclass() function, but the registering ABC won't show up in
-        their MRO (Method Resolution Order) nor will method
-        implementations defined by the registering ABC be callable (not
-        even via super()).
-        """
-        def __new__(mcls, name, bases, namespace, **kwargs):
-            cls = super().__new__(mcls, name, bases, namespace, **kwargs)
-            _abc_init(cls)
-            return cls
-
-        def register(cls, subclass):
-            """Register a virtual subclass of an ABC.
-
-            Returns the subclass, to allow usage as a class decorator.
-            """
-            return _abc_register(cls, subclass)
-
-        def __instancecheck__(cls, instance):
-            """Override for isinstance(instance, cls)."""
-            return _abc_instancecheck(cls, instance)
-
-        def __subclasscheck__(cls, subclass):
-            """Override for issubclass(subclass, cls)."""
-            return _abc_subclasscheck(cls, subclass)
-
-        def _dump_registry(cls, file=None):
-            """Debug helper to print the ABC registry."""
-            print(f"Class: {cls.__module__}.{cls.__qualname__}", file=file)
-            print(f"Inv. counter: {get_cache_token()}", file=file)
-            (_abc_registry, _abc_cache, _abc_negative_cache,
-             _abc_negative_cache_version) = _get_dump(cls)
-            print(f"_abc_registry: {_abc_registry!r}", file=file)
-            print(f"_abc_cache: {_abc_cache!r}", file=file)
-            print(f"_abc_negative_cache: {_abc_negative_cache!r}", file=file)
-            print(f"_abc_negative_cache_version: {_abc_negative_cache_version!r}",
-                  file=file)
-
-        def _abc_registry_clear(cls):
-            """Clear the registry (for debugging or testing)."""
-            _reset_registry(cls)
-
-        def _abc_caches_clear(cls):
-            """Clear the caches (for debugging or testing)."""
-            _reset_caches(cls)
-
-
-class ABC(metaclass=ABCMeta):
-    """Helper class that provides a standard way to create an ABC using
-    inheritance.
-    """
-    __slots__ = ()
diff --git a/A2SP_out/used_configs/SymbolicCoordinatePPO/2023-02-01_07-28-00/config_kwargs.json b/A2SP_out/used_configs/SymbolicCoordinatePPO/2023-02-01_07-28-00/config_kwargs.json
deleted file mode 100644
index 9e26dfee..00000000
--- a/A2SP_out/used_configs/SymbolicCoordinatePPO/2023-02-01_07-28-00/config_kwargs.json
+++ /dev/null
@@ -1 +0,0 @@
-{}
\ No newline at end of file
diff --git a/A2SP_out/used_configs/SymbolicCoordinatePPO/2023-02-01_07-28-00/coordinate_base.py b/A2SP_out/used_configs/SymbolicCoordinatePPO/2023-02-01_07-28-00/coordinate_base.py
deleted file mode 100644
index 0959bcc3..00000000
--- a/A2SP_out/used_configs/SymbolicCoordinatePPO/2023-02-01_07-28-00/coordinate_base.py
+++ /dev/null
@@ -1,470 +0,0 @@
-### THIS FILE ORIGINALLY LOCATED AT '/home/zidong/work/allenact/projects/ithor_A2SP/configs/coordinate_base.py'
-
-import copy
-import platform
-from abc import abstractmethod
-from typing import Optional, List, Sequence, Dict, Any, Tuple
-
-import ai2thor.platform
-import gym.spaces
-import stringcase
-import torch
-import torchvision.models
-from torch import nn, cuda, optim
-from torch.optim.lr_scheduler import LambdaLR
-
-from allenact.base_abstractions.experiment_config import (
-    ExperimentConfig,
-    MachineParams,
-    split_processes_onto_devices,
-)
-
-from allenact.base_abstractions.preprocessor import SensorPreprocessorGraph
-from allenact.base_abstractions.sensor import SensorSuite, Sensor, ExpertActionSensor
-from allenact.embodiedai.preprocessors.resnet import ResNetPreprocessor
-from allenact.utils.experiment_utils import TrainingPipeline, LinearDecay, Builder
-from allenact.utils.misc_utils import partition_sequence, md5_hash_str_as_int
-from allenact.utils.system import get_logger
-from allenact_plugins.ithor_plugin.ithor_util import get_open_x_displays
-
-
-import data_generator.gen_data_utils as gen_data_utils
-
-from A2SP.multi_agent_env import MultiAgentEvneronment
-from A2SP.constants import (
-    AgentType,
-    Mode,
-    THOR_COMMIT_ID, 
-    VISIBILITY_DISTANCE,
-    SCREEN_SIZE,
-    ACTION_SPACE,
-    USED_OBJECTS_TYPE_WITH_PROPERTIES_IN_KITCHENS
-)
-from A2SP.models import CoordinateActorCristicRNN
-from A2SP.tasks import CoordinateTaskSampler
-
-class CoordinateBaseExperimentConfig(ExperimentConfig):
-    # Task parameters
-    MAX_STEPS = 200
-    REQUIRE_DONE_ACTION = False
-
-    # Environment parameters
-    MODE = Mode.SYMBOLIC
-    AGENTS_NUM = 2
-    MA_ENV_KWARGS = {
-        'agents_type': [AgentType.AGENT_WITH_FULL_CAPABILITIES, AgentType.AGENT_WITH_FULL_CAPABILITIES],
-        'main_agent_id': 0,
-        'agents_num': AGENTS_NUM,
-        'mode': MODE,
-    }
-
-    # controller parameters
-    THOR_CONTROLLER_KWARGS = dict(
-        # controller setting
-        commit_id = THOR_COMMIT_ID,
-        fastActionEmit = True,
-        # agent configuration 
-        agentCount = AGENTS_NUM,
-        agentMode="default",
-        visibilityDistance=VISIBILITY_DISTANCE,
-        # Navigation properties
-        gridSize=0.25,
-        snapToGrid=True,
-        rotateStepDegrees=90,
-        # camera properties
-        width=SCREEN_SIZE,
-        height=SCREEN_SIZE,
-        fieldOfView=90,
-        # image modalities
-        renderDepthImage=True,
-        renderInstanceSegmentation=True,
-    )
-
-    # Model parameters
-    AGENTS_EMBED_LENGTH = 64
-    HIDDEN_SIZE = 512
-    RNN_LAYERS_NUM = 1
-    RNN_TYPE = 'GRU'
-
-
-    # Training parameters
-    TRAINING_STEPS = int(75e6)
-    SAVE_INTERVAL = int(1e6)
-
-    # Sensor info
-    SENSORS: Optional[Sequence[Sensor]] = None
-
-    # Actions
-    PICKUP_ACTIONS = list(
-        sorted(
-            [
-                f'PickUp={stringcase.capitalcase(obj_type)}'
-                for obj_type in USED_OBJECTS_TYPE_WITH_PROPERTIES_IN_KITCHENS['pickupable']
-            ]
-        )
-    )
-    PUT_ACTIONS = list(
-        sorted(
-            [
-                f'Put={stringcase.capitalcase(obj_type)}'
-                for obj_type in USED_OBJECTS_TYPE_WITH_PROPERTIES_IN_KITCHENS['receptacle']
-            ]
-        )
-    )
-    OPEN_ACTIONS = list(
-        sorted(
-            [
-                f'Open={stringcase.capitalcase(obj_type)}'
-                for obj_type in USED_OBJECTS_TYPE_WITH_PROPERTIES_IN_KITCHENS['openable']
-            ]
-        )
-    )
-
-    @classmethod
-    def sensors(cls) -> Sequence[Sensor]:
-        return cls.SENSORS
-
-    @classmethod
-    def actions(cls):
-        function_actions = ['Wait', 'Done'] if cls.REQUIRE_DONE_ACTION else ['Wait'] 
-        return (
-            *function_actions,
-            *ACTION_SPACE['navigation_actions'],
-            *cls.PICKUP_ACTIONS,
-            *cls.PUT_ACTIONS,
-            *cls.OPEN_ACTIONS,
-        )
-
-    @classmethod
-    def get_lr_scheduler_builder(cls, use_lr_decay: bool):
-        return (
-            None
-            if not use_lr_decay
-            else Builder(
-                LambdaLR,
-                {
-                    "lr_lambda": LinearDecay(
-                        steps=cls.TRAINING_STEPS // 3, startp=1.0, endp=1.0 / 3
-                    )
-                },
-            )
-        )
-
-    @classmethod
-    def machine_params(cls, mode="train", **kwargs) -> MachineParams:
-        '''
-        Return the number of processes and gpu_ids to use with training.
-        '''
-        num_gpus = cuda.device_count()
-        has_gpu = num_gpus != 0
-
-        sampler_devices = None
-        if mode == "train":
-            nprocesses = cls.num_train_processes() if torch.cuda.is_available() else 1
-            devices = (
-                list(range(min(nprocesses, num_gpus)))
-                if has_gpu
-                else [torch.device("cpu")]
-            )
-        elif mode == "valid":
-            devices = [num_gpus - 1] if has_gpu else [torch.device("cpu")]
-            nprocesses = 2 if has_gpu else 0
-        else:
-            nprocesses = 2 if has_gpu else 1
-            devices = (
-                list(range(min(nprocesses, num_gpus)))
-                if has_gpu
-                else [torch.device("cpu")]
-            )
-
-        nprocesses = split_processes_onto_devices(
-            nprocesses=nprocesses, ndevices=len(devices)
-        )
-
-        return MachineParams(
-            nprocesses=nprocesses,
-            devices=devices,
-            sampler_devices=sampler_devices,
-        )
- 
-    @classmethod
-    def stagewise_task_sampler_args(
-        cls,
-        stage: str,
-        process_ind: int,
-        total_processes: int,
-        allowed_ids_subset: Optional[Sequence[int]] = None,
-        allowed_scenes: Sequence[str] = None,
-        devices: Optional[List[int]] = None,
-        seeds: Optional[List[int]] = None,
-        deterministic_cudnn: bool = False,
-    ):
-        if allowed_scenes is not None:
-            scenes = allowed_scenes
-        elif stage == "combined":
-            # Split scenes more evenly as the train scenes will have more episodes
-            train_scenes = gen_data_utils.get_scenes("train")
-            other_scenes = gen_data_utils.get_scenes("val") + gen_data_utils.get_scenes(
-                "test"
-            )
-            assert len(train_scenes) == 2 * len(other_scenes)
-            scenes = []
-            while len(train_scenes) != 0:
-                scenes.append(train_scenes.pop())
-                scenes.append(train_scenes.pop())
-                scenes.append(other_scenes.pop())
-            assert len(train_scenes) == len(other_scenes)
-        else:
-            scenes = gen_data_utils.get_scenes(stage)
-
-        if total_processes > len(scenes):
-            assert stage == "train" and total_processes % len(scenes) == 0
-            scenes = scenes * (total_processes // len(scenes))
-
-        allowed_scenes = list(
-            sorted(partition_sequence(seq=scenes, parts=total_processes,)[process_ind])
-        )
-
-        scene_to_allowed_ids = None
-        if allowed_ids_subset is not None:
-            allowed_ids_subset = tuple(allowed_ids_subset)
-            assert stage in ["valid", "train_unseen"]
-            scene_to_allowed_ids = {
-                scene: allowed_ids_subset for scene in allowed_scenes
-            }
-        seed = md5_hash_str_as_int(str(allowed_scenes))
-
-        device = (
-            devices[process_ind % len(devices)]
-            if devices is not None and len(devices) > 0
-            else torch.device("cpu")
-        )
-        x_display: Optional[str] = None
-        gpu_device: Optional[int] = None
-        thor_platform: Optional[ai2thor.platform.BaseLinuxPlatform] = None
-        if platform.system() == "Linux":
-            try:
-                x_displays = get_open_x_displays(throw_error_if_empty=True)
-             
-                if devices is not None and len(
-                    [d for d in devices if d != torch.device("cpu")]
-                ) > len(x_displays):
-                    get_logger().warning(
-                        f"More GPU devices found than X-displays (devices: `{x_displays}`, x_displays: `{x_displays}`)."
-                        f" This is not necessarily a bad thing but may mean that you're not using GPU memory as"
-                        f" efficiently as possible. Consider following the instructions here:"
-                        f" https://allenact.org/installation/installation-framework/#installation-of-ithor-ithor-plugin"
-                        f" describing how to start an X-display on every GPU."
-                    )
-                x_display = x_displays[process_ind % len(x_displays)]
-            except IOError: # Actually, CloudRendering is used on severs. 
-                # Could not find an open `x_display`, use CloudRendering instead.
-                assert all(
-                    [d != torch.device("cpu") and d >= 0 for d in devices]
-                ), "Cannot use CPU devices when there are no open x-displays as CloudRendering requires specifying a GPU."
-                gpu_device = device
-                thor_platform = ai2thor.platform.CloudRendering
-
-        kwargs = {
-            "stage": stage,
-            "allowed_scenes": allowed_scenes,
-            "scene_to_allowed_ids": scene_to_allowed_ids,
-            "seed": seed,
-            "x_display": x_display,
-            "thor_controller_kwargs": {
-                "gpu_device": gpu_device,
-                "platform": thor_platform,
-            },
-        }
-        
-        sensors = kwargs.get("sensors", copy.deepcopy(cls.sensors()))
-        kwargs["sensors"] = sensors
-
-        return kwargs
-
-    @classmethod
-    def train_task_sampler_args(
-        cls,
-        process_ind: int,
-        total_processes: int,
-        devices: Optional[List[int]] = None,
-        seeds: Optional[List[int]] = None,
-        deterministic_cudnn: bool = False,
-    ):
-        return dict(
-            force_cache_reset=False,
-            epochs=float("inf"),
-            **cls.stagewise_task_sampler_args(
-                stage="train",
-                process_ind=process_ind,
-                total_processes=total_processes,
-                devices=devices,
-                seeds=seeds,
-                deterministic_cudnn=deterministic_cudnn,
-            ),
-        )
-
-    @classmethod
-    def valid_task_sampler_args(
-        cls,
-        process_ind: int,
-        total_processes: int,
-        devices: Optional[List[int]] = None,
-        seeds: Optional[List[int]] = None,
-        deterministic_cudnn: bool = False,
-    ):
-        return dict(
-            force_cache_reset=True,
-            epochs=1,
-            **cls.stagewise_task_sampler_args(
-                stage="valid",
-                allowed_rearrange_inds_subset=tuple(range(0, 50, 5)),
-                process_ind=process_ind,
-                total_processes=total_processes,
-                devices=devices,
-                seeds=seeds,
-                deterministic_cudnn=deterministic_cudnn,
-            ),
-        )
-
-    @classmethod
-    def test_task_sampler_args(
-        cls,
-        process_ind: int,
-        total_processes: int,
-        devices: Optional[List[int]] = None,
-        seeds: Optional[List[int]] = None,
-        deterministic_cudnn: bool = False,
-        task_spec_in_metrics: bool = False,
-    ):
-        task_spec_in_metrics = False
-
-        # Train_unseen
-        # stage = "train_unseen"
-        # allowed_rearrange_inds_subset = list(range(15))
-
-        # Val
-        # stage = "val"
-        # allowed_rearrange_inds_subset = None
-
-        # Test
-        # stage = "test"
-        # allowed_rearrange_inds_subset = None
-
-        # Combined (Will run inference on all datasets)
-        stage = "combined"
-        allowed_rearrange_inds_subset = None
-
-        return dict(
-            force_cache_reset=True,
-            epochs=1,
-            task_spec_in_metrics=task_spec_in_metrics,
-            **cls.stagewise_task_sampler_args(
-                stage=stage,
-                allowed_rearrange_inds_subset=allowed_rearrange_inds_subset,
-                process_ind=process_ind,
-                total_processes=total_processes,
-                devices=devices,
-                seeds=seeds,
-                deterministic_cudnn=deterministic_cudnn,
-            ),
-        )
-
-    @classmethod
-    def make_sampler_fn(
-        cls,
-        stage: str,
-        force_cache_reset: bool,
-        allowed_scenes: Optional[Sequence[str]],
-        seed: int,
-        epochs: int,
-        scene_to_allowed_ids: Optional[Dict[str, Sequence[int]]] = None,
-        x_display: Optional[str] = None,
-        sensors: Optional[Sequence[Sensor]] = None,
-        thor_controller_kwargs: Optional[Dict] = None,
-        **kwargs,
-    ) -> CoordinateTaskSampler:
-        '''
-        Return a CoordinateTaskSampler.
-        '''
-        sensors = cls.sensors() if sensors is None else sensors
-        if "mp_ctx" in kwargs:
-            del kwargs["mp_ctx"]
-        
-        return CoordinateTaskSampler.from_fixed_dataset(
-            stage=stage,
-            allowed_scenes=allowed_scenes,
-            scene_to_allowed_ids=scene_to_allowed_ids,
-            ma_env_kwargs=dict(
-                **cls.MA_ENV_KWARGS,
-                controller_kwargs={
-                    "x_display": x_display,
-                    **cls.THOR_CONTROLLER_KWARGS,
-                    **(
-                        {} if thor_controller_kwargs is None else thor_controller_kwargs
-                    ),
-                },
-            ),
-            seed=seed,
-            sensors=SensorSuite(sensors),
-            max_steps=cls.MAX_STEPS,
-            discrete_actions=cls.actions(),
-            require_done_action=cls.REQUIRE_DONE_ACTION,
-            epochs=epochs,
-            **kwargs,
-        )
-
-    @classmethod
-    @abstractmethod
-    def _training_pipeline_info(cls) -> Dict[str, Any]: # This function is defined in the subclasses
-        raise NotImplementedError
-
-    @classmethod
-    @abstractmethod
-    def num_train_processes(cls) -> int:    # This function is defined in the subclasses
-        raise NotImplementedError
-
-    @classmethod
-    def training_pipeline(cls, **kwargs) -> TrainingPipeline:
-        info = cls._training_pipeline_info()
-
-        return TrainingPipeline(
-            gamma=info.get("gamma", 0.99),
-            use_gae=info.get("use_gae", True),
-            gae_lambda=info.get("gae_lambda", 0.95),
-            num_steps=info["num_steps"],
-            num_mini_batch=info["num_mini_batch"],
-            update_repeats=info["update_repeats"],
-            max_grad_norm=info.get("max_grad_norm", 0.5),
-            save_interval=cls.SAVE_INTERVAL,
-            named_losses=info["named_losses"],
-            metric_accumulate_interval=(
-                cls.num_train_processes() * cls.MAX_STEPS
-                if torch.cuda.is_available()
-                else 1
-            ),
-            optimizer_builder=Builder(optim.Adam, dict(lr=info["lr"])),
-            advance_scene_rollout_period=None,
-            pipeline_stages=info["pipeline_stages"],
-            lr_scheduler_builder=cls.get_lr_scheduler_builder(
-                use_lr_decay=info["use_lr_decay"]
-            ),
-        )
-
-    @classmethod
-    def create_model(cls, **kwargs) -> nn.Module:
-        return CoordinateActorCristicRNN(
-            # action_space=gym.spaces.Tuple(
-            #     (
-            #         gym.spaces.Discrete(len(cls.actions())),
-            #         gym.spaces.Discrete(len(cls.actions()))
-            #     )
-            # ), 
-            action_space=gym.spaces.Discrete(len(cls.actions())),
-            observation_space=SensorSuite(cls.sensors()).observation_spaces,
-            agents_num=cls.AGENTS_NUM,
-            agents_embed_length=cls.AGENTS_EMBED_LENGTH,
-            hidden_size=cls.HIDDEN_SIZE,
-            rnn_layers_num=cls.RNN_LAYERS_NUM,
-            rnn_type=cls.RNN_TYPE
-        )
\ No newline at end of file
diff --git a/A2SP_out/used_configs/SymbolicCoordinatePPO/2023-02-01_07-28-00/symbolic_base.py b/A2SP_out/used_configs/SymbolicCoordinatePPO/2023-02-01_07-28-00/symbolic_base.py
deleted file mode 100644
index 63dfd68a..00000000
--- a/A2SP_out/used_configs/SymbolicCoordinatePPO/2023-02-01_07-28-00/symbolic_base.py
+++ /dev/null
@@ -1,23 +0,0 @@
-### THIS FILE ORIGINALLY LOCATED AT '/home/zidong/work/allenact/projects/ithor_A2SP/configs/symbolic_representation/symbolic_base.py'
-
-from abc import ABC
-from typing import Optional, Dict, Sequence
-
-from allenact.base_abstractions.sensor import SensorSuite, Sensor
-
-
-from configs.coordinate_base import CoordinateBaseExperimentConfig
-from A2SP.sensors import SymbolicObjectSensor
-from A2SP.constants import Mode
-
-class SymbolicCoordinateExperimentConfig(CoordinateBaseExperimentConfig, ABC):
-    MODE = Mode.SYMBOLIC
-
-    @classmethod
-    def sensors(cls) -> Sequence[Sensor]:
-        return [
-            SymbolicObjectSensor()
-        ]
-
-
-
diff --git a/A2SP_out/used_configs/SymbolicCoordinatePPO/2023-02-01_07-28-00/symbolic_base_ppo.py b/A2SP_out/used_configs/SymbolicCoordinatePPO/2023-02-01_07-28-00/symbolic_base_ppo.py
deleted file mode 100644
index f01c5cee..00000000
--- a/A2SP_out/used_configs/SymbolicCoordinatePPO/2023-02-01_07-28-00/symbolic_base_ppo.py
+++ /dev/null
@@ -1,46 +0,0 @@
-### THIS FILE ORIGINALLY LOCATED AT '/home/zidong/work/allenact/projects/ithor_A2SP/configs/symbolic_representation/symbolic_base_ppo.py'
-
-from typing import Dict, Any
-
-from allenact.algorithms.onpolicy_sync.losses import PPO
-from allenact.algorithms.onpolicy_sync.losses.ppo import PPOConfig
-from allenact.utils.experiment_utils import LinearDecay, PipelineStage
-
-import sys
-import os
-sys.path.append(os.path.join(os.path.abspath('.'), 'projects/ithor_A2SP'))
-print(os.path.join(os.path.abspath('.'), 'projects/ithor_A2SP'))
-
-
-from configs.symbolic_representation.symbolic_base import SymbolicCoordinateExperimentConfig
-
-class SymbolicCoordinatePPOExperimentConfig(SymbolicCoordinateExperimentConfig):
-    @classmethod
-    def tag(cls) -> str:
-        return "SymbolicCoordinatePPO"
-
-    @classmethod
-    def num_train_processes(cls) -> int:
-        return 2
-
-    @classmethod
-    def _training_pipeline_info(cls, **kwargs) -> Dict[str, Any]:
-        '''
-        Define how the model trains.
-        '''
-
-        training_steps = cls.TRAINING_STEPS
-        return dict(
-            named_losses=dict(
-                ppo_loss=PPO(clip_decay=LinearDecay(training_steps), **PPOConfig)
-            ),
-            pipeline_stages=[
-                PipelineStage(loss_names=["ppo_loss"], max_stage_steps=training_steps,)
-            ],
-            num_steps=64,
-            num_mini_batch=1,
-            update_repeats=3,
-            use_lr_decay=True,
-            lr=3e-4,
-        )
-
diff --git a/A2SP_out/used_configs/SymbolicCoordinatePPO/2023-02-01_07-29-59/3d4aade3.patch b/A2SP_out/used_configs/SymbolicCoordinatePPO/2023-02-01_07-29-59/3d4aade3.patch
deleted file mode 100644
index 9bf7c7d0..00000000
--- a/A2SP_out/used_configs/SymbolicCoordinatePPO/2023-02-01_07-29-59/3d4aade3.patch
+++ /dev/null
@@ -1,23 +0,0 @@
-diff --git a/main.py b/main.py
-index 22ef3f2a..fae72ce7 100755
---- a/main.py
-+++ b/main.py
-@@ -3,5 +3,12 @@
- name."""
- import allenact.main
- 
-+import sys
-+import os
-+sys.path.append(os.path.join(os.path.abspath('.'), 'projects/ithor_A2SP'))
-+print(os.path.join(os.path.abspath('.'), 'projects/ithor_A2SP'))
-+
-+
-+
- if __name__ == "__main__":
-     allenact.main.main()
-diff --git a/projects/ithor_A2SP b/projects/ithor_A2SP
---- a/projects/ithor_A2SP
-+++ b/projects/ithor_A2SP
-@@ -1 +1 @@
--Subproject commit c234a1827742cca4ad591bb19c0282b566a54122
-+Subproject commit c234a1827742cca4ad591bb19c0282b566a54122-dirty
diff --git a/A2SP_out/used_configs/SymbolicCoordinatePPO/2023-02-01_07-29-59/abc.py b/A2SP_out/used_configs/SymbolicCoordinatePPO/2023-02-01_07-29-59/abc.py
deleted file mode 100644
index cfabe90b..00000000
--- a/A2SP_out/used_configs/SymbolicCoordinatePPO/2023-02-01_07-29-59/abc.py
+++ /dev/null
@@ -1,131 +0,0 @@
-### THIS FILE ORIGINALLY LOCATED AT '/home/zidong/anaconda3/envs/A2SP_env/lib/python3.8/abc.py'
-
-# Copyright 2007 Google, Inc. All Rights Reserved.
-# Licensed to PSF under a Contributor Agreement.
-
-"""Abstract Base Classes (ABCs) according to PEP 3119."""
-
-
-def abstractmethod(funcobj):
-    """A decorator indicating abstract methods.
-
-    Requires that the metaclass is ABCMeta or derived from it.  A
-    class that has a metaclass derived from ABCMeta cannot be
-    instantiated unless all of its abstract methods are overridden.
-    The abstract methods can be called using any of the normal
-    'super' call mechanisms.  abstractmethod() may be used to declare
-    abstract methods for properties and descriptors.
-
-    Usage:
-
-        class C(metaclass=ABCMeta):
-            @abstractmethod
-            def my_abstract_method(self, ...):
-                ...
-    """
-    funcobj.__isabstractmethod__ = True
-    return funcobj
-
-
-class abstractclassmethod(classmethod):
-    """A decorator indicating abstract classmethods.
-
-    Deprecated, use 'classmethod' with 'abstractmethod' instead.
-    """
-
-    __isabstractmethod__ = True
-
-    def __init__(self, callable):
-        callable.__isabstractmethod__ = True
-        super().__init__(callable)
-
-
-class abstractstaticmethod(staticmethod):
-    """A decorator indicating abstract staticmethods.
-
-    Deprecated, use 'staticmethod' with 'abstractmethod' instead.
-    """
-
-    __isabstractmethod__ = True
-
-    def __init__(self, callable):
-        callable.__isabstractmethod__ = True
-        super().__init__(callable)
-
-
-class abstractproperty(property):
-    """A decorator indicating abstract properties.
-
-    Deprecated, use 'property' with 'abstractmethod' instead.
-    """
-
-    __isabstractmethod__ = True
-
-
-try:
-    from _abc import (get_cache_token, _abc_init, _abc_register,
-                      _abc_instancecheck, _abc_subclasscheck, _get_dump,
-                      _reset_registry, _reset_caches)
-except ImportError:
-    from _py_abc import ABCMeta, get_cache_token
-    ABCMeta.__module__ = 'abc'
-else:
-    class ABCMeta(type):
-        """Metaclass for defining Abstract Base Classes (ABCs).
-
-        Use this metaclass to create an ABC.  An ABC can be subclassed
-        directly, and then acts as a mix-in class.  You can also register
-        unrelated concrete classes (even built-in classes) and unrelated
-        ABCs as 'virtual subclasses' -- these and their descendants will
-        be considered subclasses of the registering ABC by the built-in
-        issubclass() function, but the registering ABC won't show up in
-        their MRO (Method Resolution Order) nor will method
-        implementations defined by the registering ABC be callable (not
-        even via super()).
-        """
-        def __new__(mcls, name, bases, namespace, **kwargs):
-            cls = super().__new__(mcls, name, bases, namespace, **kwargs)
-            _abc_init(cls)
-            return cls
-
-        def register(cls, subclass):
-            """Register a virtual subclass of an ABC.
-
-            Returns the subclass, to allow usage as a class decorator.
-            """
-            return _abc_register(cls, subclass)
-
-        def __instancecheck__(cls, instance):
-            """Override for isinstance(instance, cls)."""
-            return _abc_instancecheck(cls, instance)
-
-        def __subclasscheck__(cls, subclass):
-            """Override for issubclass(subclass, cls)."""
-            return _abc_subclasscheck(cls, subclass)
-
-        def _dump_registry(cls, file=None):
-            """Debug helper to print the ABC registry."""
-            print(f"Class: {cls.__module__}.{cls.__qualname__}", file=file)
-            print(f"Inv. counter: {get_cache_token()}", file=file)
-            (_abc_registry, _abc_cache, _abc_negative_cache,
-             _abc_negative_cache_version) = _get_dump(cls)
-            print(f"_abc_registry: {_abc_registry!r}", file=file)
-            print(f"_abc_cache: {_abc_cache!r}", file=file)
-            print(f"_abc_negative_cache: {_abc_negative_cache!r}", file=file)
-            print(f"_abc_negative_cache_version: {_abc_negative_cache_version!r}",
-                  file=file)
-
-        def _abc_registry_clear(cls):
-            """Clear the registry (for debugging or testing)."""
-            _reset_registry(cls)
-
-        def _abc_caches_clear(cls):
-            """Clear the caches (for debugging or testing)."""
-            _reset_caches(cls)
-
-
-class ABC(metaclass=ABCMeta):
-    """Helper class that provides a standard way to create an ABC using
-    inheritance.
-    """
-    __slots__ = ()
diff --git a/A2SP_out/used_configs/SymbolicCoordinatePPO/2023-02-01_07-29-59/config_kwargs.json b/A2SP_out/used_configs/SymbolicCoordinatePPO/2023-02-01_07-29-59/config_kwargs.json
deleted file mode 100644
index 9e26dfee..00000000
--- a/A2SP_out/used_configs/SymbolicCoordinatePPO/2023-02-01_07-29-59/config_kwargs.json
+++ /dev/null
@@ -1 +0,0 @@
-{}
\ No newline at end of file
diff --git a/A2SP_out/used_configs/SymbolicCoordinatePPO/2023-02-01_07-29-59/coordinate_base.py b/A2SP_out/used_configs/SymbolicCoordinatePPO/2023-02-01_07-29-59/coordinate_base.py
deleted file mode 100644
index 0959bcc3..00000000
--- a/A2SP_out/used_configs/SymbolicCoordinatePPO/2023-02-01_07-29-59/coordinate_base.py
+++ /dev/null
@@ -1,470 +0,0 @@
-### THIS FILE ORIGINALLY LOCATED AT '/home/zidong/work/allenact/projects/ithor_A2SP/configs/coordinate_base.py'
-
-import copy
-import platform
-from abc import abstractmethod
-from typing import Optional, List, Sequence, Dict, Any, Tuple
-
-import ai2thor.platform
-import gym.spaces
-import stringcase
-import torch
-import torchvision.models
-from torch import nn, cuda, optim
-from torch.optim.lr_scheduler import LambdaLR
-
-from allenact.base_abstractions.experiment_config import (
-    ExperimentConfig,
-    MachineParams,
-    split_processes_onto_devices,
-)
-
-from allenact.base_abstractions.preprocessor import SensorPreprocessorGraph
-from allenact.base_abstractions.sensor import SensorSuite, Sensor, ExpertActionSensor
-from allenact.embodiedai.preprocessors.resnet import ResNetPreprocessor
-from allenact.utils.experiment_utils import TrainingPipeline, LinearDecay, Builder
-from allenact.utils.misc_utils import partition_sequence, md5_hash_str_as_int
-from allenact.utils.system import get_logger
-from allenact_plugins.ithor_plugin.ithor_util import get_open_x_displays
-
-
-import data_generator.gen_data_utils as gen_data_utils
-
-from A2SP.multi_agent_env import MultiAgentEvneronment
-from A2SP.constants import (
-    AgentType,
-    Mode,
-    THOR_COMMIT_ID, 
-    VISIBILITY_DISTANCE,
-    SCREEN_SIZE,
-    ACTION_SPACE,
-    USED_OBJECTS_TYPE_WITH_PROPERTIES_IN_KITCHENS
-)
-from A2SP.models import CoordinateActorCristicRNN
-from A2SP.tasks import CoordinateTaskSampler
-
-class CoordinateBaseExperimentConfig(ExperimentConfig):
-    # Task parameters
-    MAX_STEPS = 200
-    REQUIRE_DONE_ACTION = False
-
-    # Environment parameters
-    MODE = Mode.SYMBOLIC
-    AGENTS_NUM = 2
-    MA_ENV_KWARGS = {
-        'agents_type': [AgentType.AGENT_WITH_FULL_CAPABILITIES, AgentType.AGENT_WITH_FULL_CAPABILITIES],
-        'main_agent_id': 0,
-        'agents_num': AGENTS_NUM,
-        'mode': MODE,
-    }
-
-    # controller parameters
-    THOR_CONTROLLER_KWARGS = dict(
-        # controller setting
-        commit_id = THOR_COMMIT_ID,
-        fastActionEmit = True,
-        # agent configuration 
-        agentCount = AGENTS_NUM,
-        agentMode="default",
-        visibilityDistance=VISIBILITY_DISTANCE,
-        # Navigation properties
-        gridSize=0.25,
-        snapToGrid=True,
-        rotateStepDegrees=90,
-        # camera properties
-        width=SCREEN_SIZE,
-        height=SCREEN_SIZE,
-        fieldOfView=90,
-        # image modalities
-        renderDepthImage=True,
-        renderInstanceSegmentation=True,
-    )
-
-    # Model parameters
-    AGENTS_EMBED_LENGTH = 64
-    HIDDEN_SIZE = 512
-    RNN_LAYERS_NUM = 1
-    RNN_TYPE = 'GRU'
-
-
-    # Training parameters
-    TRAINING_STEPS = int(75e6)
-    SAVE_INTERVAL = int(1e6)
-
-    # Sensor info
-    SENSORS: Optional[Sequence[Sensor]] = None
-
-    # Actions
-    PICKUP_ACTIONS = list(
-        sorted(
-            [
-                f'PickUp={stringcase.capitalcase(obj_type)}'
-                for obj_type in USED_OBJECTS_TYPE_WITH_PROPERTIES_IN_KITCHENS['pickupable']
-            ]
-        )
-    )
-    PUT_ACTIONS = list(
-        sorted(
-            [
-                f'Put={stringcase.capitalcase(obj_type)}'
-                for obj_type in USED_OBJECTS_TYPE_WITH_PROPERTIES_IN_KITCHENS['receptacle']
-            ]
-        )
-    )
-    OPEN_ACTIONS = list(
-        sorted(
-            [
-                f'Open={stringcase.capitalcase(obj_type)}'
-                for obj_type in USED_OBJECTS_TYPE_WITH_PROPERTIES_IN_KITCHENS['openable']
-            ]
-        )
-    )
-
-    @classmethod
-    def sensors(cls) -> Sequence[Sensor]:
-        return cls.SENSORS
-
-    @classmethod
-    def actions(cls):
-        function_actions = ['Wait', 'Done'] if cls.REQUIRE_DONE_ACTION else ['Wait'] 
-        return (
-            *function_actions,
-            *ACTION_SPACE['navigation_actions'],
-            *cls.PICKUP_ACTIONS,
-            *cls.PUT_ACTIONS,
-            *cls.OPEN_ACTIONS,
-        )
-
-    @classmethod
-    def get_lr_scheduler_builder(cls, use_lr_decay: bool):
-        return (
-            None
-            if not use_lr_decay
-            else Builder(
-                LambdaLR,
-                {
-                    "lr_lambda": LinearDecay(
-                        steps=cls.TRAINING_STEPS // 3, startp=1.0, endp=1.0 / 3
-                    )
-                },
-            )
-        )
-
-    @classmethod
-    def machine_params(cls, mode="train", **kwargs) -> MachineParams:
-        '''
-        Return the number of processes and gpu_ids to use with training.
-        '''
-        num_gpus = cuda.device_count()
-        has_gpu = num_gpus != 0
-
-        sampler_devices = None
-        if mode == "train":
-            nprocesses = cls.num_train_processes() if torch.cuda.is_available() else 1
-            devices = (
-                list(range(min(nprocesses, num_gpus)))
-                if has_gpu
-                else [torch.device("cpu")]
-            )
-        elif mode == "valid":
-            devices = [num_gpus - 1] if has_gpu else [torch.device("cpu")]
-            nprocesses = 2 if has_gpu else 0
-        else:
-            nprocesses = 2 if has_gpu else 1
-            devices = (
-                list(range(min(nprocesses, num_gpus)))
-                if has_gpu
-                else [torch.device("cpu")]
-            )
-
-        nprocesses = split_processes_onto_devices(
-            nprocesses=nprocesses, ndevices=len(devices)
-        )
-
-        return MachineParams(
-            nprocesses=nprocesses,
-            devices=devices,
-            sampler_devices=sampler_devices,
-        )
- 
-    @classmethod
-    def stagewise_task_sampler_args(
-        cls,
-        stage: str,
-        process_ind: int,
-        total_processes: int,
-        allowed_ids_subset: Optional[Sequence[int]] = None,
-        allowed_scenes: Sequence[str] = None,
-        devices: Optional[List[int]] = None,
-        seeds: Optional[List[int]] = None,
-        deterministic_cudnn: bool = False,
-    ):
-        if allowed_scenes is not None:
-            scenes = allowed_scenes
-        elif stage == "combined":
-            # Split scenes more evenly as the train scenes will have more episodes
-            train_scenes = gen_data_utils.get_scenes("train")
-            other_scenes = gen_data_utils.get_scenes("val") + gen_data_utils.get_scenes(
-                "test"
-            )
-            assert len(train_scenes) == 2 * len(other_scenes)
-            scenes = []
-            while len(train_scenes) != 0:
-                scenes.append(train_scenes.pop())
-                scenes.append(train_scenes.pop())
-                scenes.append(other_scenes.pop())
-            assert len(train_scenes) == len(other_scenes)
-        else:
-            scenes = gen_data_utils.get_scenes(stage)
-
-        if total_processes > len(scenes):
-            assert stage == "train" and total_processes % len(scenes) == 0
-            scenes = scenes * (total_processes // len(scenes))
-
-        allowed_scenes = list(
-            sorted(partition_sequence(seq=scenes, parts=total_processes,)[process_ind])
-        )
-
-        scene_to_allowed_ids = None
-        if allowed_ids_subset is not None:
-            allowed_ids_subset = tuple(allowed_ids_subset)
-            assert stage in ["valid", "train_unseen"]
-            scene_to_allowed_ids = {
-                scene: allowed_ids_subset for scene in allowed_scenes
-            }
-        seed = md5_hash_str_as_int(str(allowed_scenes))
-
-        device = (
-            devices[process_ind % len(devices)]
-            if devices is not None and len(devices) > 0
-            else torch.device("cpu")
-        )
-        x_display: Optional[str] = None
-        gpu_device: Optional[int] = None
-        thor_platform: Optional[ai2thor.platform.BaseLinuxPlatform] = None
-        if platform.system() == "Linux":
-            try:
-                x_displays = get_open_x_displays(throw_error_if_empty=True)
-             
-                if devices is not None and len(
-                    [d for d in devices if d != torch.device("cpu")]
-                ) > len(x_displays):
-                    get_logger().warning(
-                        f"More GPU devices found than X-displays (devices: `{x_displays}`, x_displays: `{x_displays}`)."
-                        f" This is not necessarily a bad thing but may mean that you're not using GPU memory as"
-                        f" efficiently as possible. Consider following the instructions here:"
-                        f" https://allenact.org/installation/installation-framework/#installation-of-ithor-ithor-plugin"
-                        f" describing how to start an X-display on every GPU."
-                    )
-                x_display = x_displays[process_ind % len(x_displays)]
-            except IOError: # Actually, CloudRendering is used on severs. 
-                # Could not find an open `x_display`, use CloudRendering instead.
-                assert all(
-                    [d != torch.device("cpu") and d >= 0 for d in devices]
-                ), "Cannot use CPU devices when there are no open x-displays as CloudRendering requires specifying a GPU."
-                gpu_device = device
-                thor_platform = ai2thor.platform.CloudRendering
-
-        kwargs = {
-            "stage": stage,
-            "allowed_scenes": allowed_scenes,
-            "scene_to_allowed_ids": scene_to_allowed_ids,
-            "seed": seed,
-            "x_display": x_display,
-            "thor_controller_kwargs": {
-                "gpu_device": gpu_device,
-                "platform": thor_platform,
-            },
-        }
-        
-        sensors = kwargs.get("sensors", copy.deepcopy(cls.sensors()))
-        kwargs["sensors"] = sensors
-
-        return kwargs
-
-    @classmethod
-    def train_task_sampler_args(
-        cls,
-        process_ind: int,
-        total_processes: int,
-        devices: Optional[List[int]] = None,
-        seeds: Optional[List[int]] = None,
-        deterministic_cudnn: bool = False,
-    ):
-        return dict(
-            force_cache_reset=False,
-            epochs=float("inf"),
-            **cls.stagewise_task_sampler_args(
-                stage="train",
-                process_ind=process_ind,
-                total_processes=total_processes,
-                devices=devices,
-                seeds=seeds,
-                deterministic_cudnn=deterministic_cudnn,
-            ),
-        )
-
-    @classmethod
-    def valid_task_sampler_args(
-        cls,
-        process_ind: int,
-        total_processes: int,
-        devices: Optional[List[int]] = None,
-        seeds: Optional[List[int]] = None,
-        deterministic_cudnn: bool = False,
-    ):
-        return dict(
-            force_cache_reset=True,
-            epochs=1,
-            **cls.stagewise_task_sampler_args(
-                stage="valid",
-                allowed_rearrange_inds_subset=tuple(range(0, 50, 5)),
-                process_ind=process_ind,
-                total_processes=total_processes,
-                devices=devices,
-                seeds=seeds,
-                deterministic_cudnn=deterministic_cudnn,
-            ),
-        )
-
-    @classmethod
-    def test_task_sampler_args(
-        cls,
-        process_ind: int,
-        total_processes: int,
-        devices: Optional[List[int]] = None,
-        seeds: Optional[List[int]] = None,
-        deterministic_cudnn: bool = False,
-        task_spec_in_metrics: bool = False,
-    ):
-        task_spec_in_metrics = False
-
-        # Train_unseen
-        # stage = "train_unseen"
-        # allowed_rearrange_inds_subset = list(range(15))
-
-        # Val
-        # stage = "val"
-        # allowed_rearrange_inds_subset = None
-
-        # Test
-        # stage = "test"
-        # allowed_rearrange_inds_subset = None
-
-        # Combined (Will run inference on all datasets)
-        stage = "combined"
-        allowed_rearrange_inds_subset = None
-
-        return dict(
-            force_cache_reset=True,
-            epochs=1,
-            task_spec_in_metrics=task_spec_in_metrics,
-            **cls.stagewise_task_sampler_args(
-                stage=stage,
-                allowed_rearrange_inds_subset=allowed_rearrange_inds_subset,
-                process_ind=process_ind,
-                total_processes=total_processes,
-                devices=devices,
-                seeds=seeds,
-                deterministic_cudnn=deterministic_cudnn,
-            ),
-        )
-
-    @classmethod
-    def make_sampler_fn(
-        cls,
-        stage: str,
-        force_cache_reset: bool,
-        allowed_scenes: Optional[Sequence[str]],
-        seed: int,
-        epochs: int,
-        scene_to_allowed_ids: Optional[Dict[str, Sequence[int]]] = None,
-        x_display: Optional[str] = None,
-        sensors: Optional[Sequence[Sensor]] = None,
-        thor_controller_kwargs: Optional[Dict] = None,
-        **kwargs,
-    ) -> CoordinateTaskSampler:
-        '''
-        Return a CoordinateTaskSampler.
-        '''
-        sensors = cls.sensors() if sensors is None else sensors
-        if "mp_ctx" in kwargs:
-            del kwargs["mp_ctx"]
-        
-        return CoordinateTaskSampler.from_fixed_dataset(
-            stage=stage,
-            allowed_scenes=allowed_scenes,
-            scene_to_allowed_ids=scene_to_allowed_ids,
-            ma_env_kwargs=dict(
-                **cls.MA_ENV_KWARGS,
-                controller_kwargs={
-                    "x_display": x_display,
-                    **cls.THOR_CONTROLLER_KWARGS,
-                    **(
-                        {} if thor_controller_kwargs is None else thor_controller_kwargs
-                    ),
-                },
-            ),
-            seed=seed,
-            sensors=SensorSuite(sensors),
-            max_steps=cls.MAX_STEPS,
-            discrete_actions=cls.actions(),
-            require_done_action=cls.REQUIRE_DONE_ACTION,
-            epochs=epochs,
-            **kwargs,
-        )
-
-    @classmethod
-    @abstractmethod
-    def _training_pipeline_info(cls) -> Dict[str, Any]: # This function is defined in the subclasses
-        raise NotImplementedError
-
-    @classmethod
-    @abstractmethod
-    def num_train_processes(cls) -> int:    # This function is defined in the subclasses
-        raise NotImplementedError
-
-    @classmethod
-    def training_pipeline(cls, **kwargs) -> TrainingPipeline:
-        info = cls._training_pipeline_info()
-
-        return TrainingPipeline(
-            gamma=info.get("gamma", 0.99),
-            use_gae=info.get("use_gae", True),
-            gae_lambda=info.get("gae_lambda", 0.95),
-            num_steps=info["num_steps"],
-            num_mini_batch=info["num_mini_batch"],
-            update_repeats=info["update_repeats"],
-            max_grad_norm=info.get("max_grad_norm", 0.5),
-            save_interval=cls.SAVE_INTERVAL,
-            named_losses=info["named_losses"],
-            metric_accumulate_interval=(
-                cls.num_train_processes() * cls.MAX_STEPS
-                if torch.cuda.is_available()
-                else 1
-            ),
-            optimizer_builder=Builder(optim.Adam, dict(lr=info["lr"])),
-            advance_scene_rollout_period=None,
-            pipeline_stages=info["pipeline_stages"],
-            lr_scheduler_builder=cls.get_lr_scheduler_builder(
-                use_lr_decay=info["use_lr_decay"]
-            ),
-        )
-
-    @classmethod
-    def create_model(cls, **kwargs) -> nn.Module:
-        return CoordinateActorCristicRNN(
-            # action_space=gym.spaces.Tuple(
-            #     (
-            #         gym.spaces.Discrete(len(cls.actions())),
-            #         gym.spaces.Discrete(len(cls.actions()))
-            #     )
-            # ), 
-            action_space=gym.spaces.Discrete(len(cls.actions())),
-            observation_space=SensorSuite(cls.sensors()).observation_spaces,
-            agents_num=cls.AGENTS_NUM,
-            agents_embed_length=cls.AGENTS_EMBED_LENGTH,
-            hidden_size=cls.HIDDEN_SIZE,
-            rnn_layers_num=cls.RNN_LAYERS_NUM,
-            rnn_type=cls.RNN_TYPE
-        )
\ No newline at end of file
diff --git a/A2SP_out/used_configs/SymbolicCoordinatePPO/2023-02-01_07-29-59/symbolic_base.py b/A2SP_out/used_configs/SymbolicCoordinatePPO/2023-02-01_07-29-59/symbolic_base.py
deleted file mode 100644
index 63dfd68a..00000000
--- a/A2SP_out/used_configs/SymbolicCoordinatePPO/2023-02-01_07-29-59/symbolic_base.py
+++ /dev/null
@@ -1,23 +0,0 @@
-### THIS FILE ORIGINALLY LOCATED AT '/home/zidong/work/allenact/projects/ithor_A2SP/configs/symbolic_representation/symbolic_base.py'
-
-from abc import ABC
-from typing import Optional, Dict, Sequence
-
-from allenact.base_abstractions.sensor import SensorSuite, Sensor
-
-
-from configs.coordinate_base import CoordinateBaseExperimentConfig
-from A2SP.sensors import SymbolicObjectSensor
-from A2SP.constants import Mode
-
-class SymbolicCoordinateExperimentConfig(CoordinateBaseExperimentConfig, ABC):
-    MODE = Mode.SYMBOLIC
-
-    @classmethod
-    def sensors(cls) -> Sequence[Sensor]:
-        return [
-            SymbolicObjectSensor()
-        ]
-
-
-
diff --git a/A2SP_out/used_configs/SymbolicCoordinatePPO/2023-02-01_07-29-59/symbolic_base_ppo.py b/A2SP_out/used_configs/SymbolicCoordinatePPO/2023-02-01_07-29-59/symbolic_base_ppo.py
deleted file mode 100644
index 6a4bd9fc..00000000
--- a/A2SP_out/used_configs/SymbolicCoordinatePPO/2023-02-01_07-29-59/symbolic_base_ppo.py
+++ /dev/null
@@ -1,40 +0,0 @@
-### THIS FILE ORIGINALLY LOCATED AT '/home/zidong/work/allenact/projects/ithor_A2SP/configs/symbolic_representation/symbolic_base_ppo.py'
-
-from typing import Dict, Any
-
-from allenact.algorithms.onpolicy_sync.losses import PPO
-from allenact.algorithms.onpolicy_sync.losses.ppo import PPOConfig
-from allenact.utils.experiment_utils import LinearDecay, PipelineStage
-
-from configs.symbolic_representation.symbolic_base import SymbolicCoordinateExperimentConfig
-
-class SymbolicCoordinatePPOExperimentConfig(SymbolicCoordinateExperimentConfig):
-    @classmethod
-    def tag(cls) -> str:
-        return "SymbolicCoordinatePPO"
-
-    @classmethod
-    def num_train_processes(cls) -> int:
-        return 2
-
-    @classmethod
-    def _training_pipeline_info(cls, **kwargs) -> Dict[str, Any]:
-        '''
-        Define how the model trains.
-        '''
-
-        training_steps = cls.TRAINING_STEPS
-        return dict(
-            named_losses=dict(
-                ppo_loss=PPO(clip_decay=LinearDecay(training_steps), **PPOConfig)
-            ),
-            pipeline_stages=[
-                PipelineStage(loss_names=["ppo_loss"], max_stage_steps=training_steps,)
-            ],
-            num_steps=64,
-            num_mini_batch=1,
-            update_repeats=3,
-            use_lr_decay=True,
-            lr=3e-4,
-        )
-
diff --git a/allenact/__pycache__/main.cpython-38.pyc b/allenact/__pycache__/main.cpython-38.pyc
index e3c49c53..80c5ffc2 100644
Binary files a/allenact/__pycache__/main.cpython-38.pyc and b/allenact/__pycache__/main.cpython-38.pyc differ
diff --git a/allenact/algorithms/onpolicy_sync/__pycache__/engine.cpython-38.pyc b/allenact/algorithms/onpolicy_sync/__pycache__/engine.cpython-38.pyc
index e4e0e5da..bf104d0d 100644
Binary files a/allenact/algorithms/onpolicy_sync/__pycache__/engine.cpython-38.pyc and b/allenact/algorithms/onpolicy_sync/__pycache__/engine.cpython-38.pyc differ
diff --git a/allenact/algorithms/onpolicy_sync/__pycache__/storage.cpython-38.pyc b/allenact/algorithms/onpolicy_sync/__pycache__/storage.cpython-38.pyc
index 0cab4a96..2b3a8f3f 100644
Binary files a/allenact/algorithms/onpolicy_sync/__pycache__/storage.cpython-38.pyc and b/allenact/algorithms/onpolicy_sync/__pycache__/storage.cpython-38.pyc differ
diff --git a/allenact/algorithms/onpolicy_sync/__pycache__/vector_sampled_tasks.cpython-38.pyc b/allenact/algorithms/onpolicy_sync/__pycache__/vector_sampled_tasks.cpython-38.pyc
index 9c415335..15ff2bfb 100644
Binary files a/allenact/algorithms/onpolicy_sync/__pycache__/vector_sampled_tasks.cpython-38.pyc and b/allenact/algorithms/onpolicy_sync/__pycache__/vector_sampled_tasks.cpython-38.pyc differ
diff --git a/allenact/algorithms/onpolicy_sync/engine.py b/allenact/algorithms/onpolicy_sync/engine.py
index f478d902..9f889539 100644
--- a/allenact/algorithms/onpolicy_sync/engine.py
+++ b/allenact/algorithms/onpolicy_sync/engine.py
@@ -80,7 +80,7 @@ except ImportError:
     DEBUGGING = str2bool(os.getenv("ALLENACT_DEBUG", "false"))
 
 DEBUG_VST_TIMEOUT: Optional[int] = (lambda x: int(x) if x is not None else x)(
-    os.getenv("ALLENACT_DEBUG_VST_TIMEOUT", None)
+    os.getenv("ALLENACT_DEBUG_VST_TIMEOUT", 5 * 60)
 )
 
 TRAIN_MODE_STR = "train"
@@ -573,7 +573,10 @@ class OnPolicyRLEngine(object):
         )
 
         # Flatten actions
-        flat_actions = su.flatten(self.actor_critic.action_space, actions)
+        if len(actions.shape) == 3:
+            flat_actions = actions
+        elif len(actions.shape) == 2:
+            flat_actions = su.flatten(self.actor_critic.action_space, actions)
 
         assert len(flat_actions.shape) == 3, (
             "Distribution samples must include step and task sampler dimensions [step, sampler, ...]. The simplest way"
diff --git a/allenact/base_abstractions/__pycache__/distributions.cpython-38.pyc b/allenact/base_abstractions/__pycache__/distributions.cpython-38.pyc
index f37ffd07..e3e32f21 100644
Binary files a/allenact/base_abstractions/__pycache__/distributions.cpython-38.pyc and b/allenact/base_abstractions/__pycache__/distributions.cpython-38.pyc differ
diff --git a/allenact/base_abstractions/distributions.py b/allenact/base_abstractions/distributions.py
index d04d98ec..b66bae0f 100644
--- a/allenact/base_abstractions/distributions.py
+++ b/allenact/base_abstractions/distributions.py
@@ -54,6 +54,8 @@ class CategoricalDistr(torch.distributions.Categorical, Distr):
         return self._param.argmax(dim=-1, keepdim=False)  # match sample()'s shape
 
     def log_prob(self, value: torch.Tensor):
+        if not isinstance(value, torch.Tensor):
+            value = torch.stack(value, dim=-1)
         if value.shape == self.logits.shape[:-1]:
             return super(CategoricalDistr, self).log_prob(value=value)
         elif value.shape == self.logits.shape[:-1] + (1,):
diff --git a/allenact/main.py b/allenact/main.py
index d1ad6d0b..d9222be5 100755
--- a/allenact/main.py
+++ b/allenact/main.py
@@ -35,8 +35,10 @@ def get_argument_parser():
     )
 
     parser.add_argument(
-        "experiment",
+        "--experiment", # add '--' to make this argument be positional
         type=str,
+        default='projects/ithor_A2SP/configs/symbolic_representation/symbolic_base_ppo.py',
+        required=False,   # for debug
         help="the path to experiment config file relative the 'experiment_base' directory"
         " (see the `--experiment_base` flag).",
     )
@@ -83,7 +85,7 @@ def get_argument_parser():
         "--output_dir",
         required=False,
         type=str,
-        default="experiment_output",
+        default="experiment_output/A2SP_out",
         help="experiment output folder",
     )
 
diff --git a/allenact/utils/__pycache__/spaces_utils.cpython-38.pyc b/allenact/utils/__pycache__/spaces_utils.cpython-38.pyc
index 44ee9d45..c2375280 100644
Binary files a/allenact/utils/__pycache__/spaces_utils.cpython-38.pyc and b/allenact/utils/__pycache__/spaces_utils.cpython-38.pyc differ
diff --git a/main.py b/main.py
index 7aec3b67..e129b43a 100755
--- a/main.py
+++ b/main.py
@@ -6,7 +6,7 @@ import allenact.main
 import sys
 import os
 sys.path.append(os.path.join(os.path.abspath('.'), 'projects/ithor_A2SP'))
-
+# os.environ["CUDA_VISIBLE_DEVICES"] = "0"
 
 if __name__ == "__main__":
     allenact.main.main()
diff --git a/projects/ithor_A2SP b/projects/ithor_A2SP
--- a/projects/ithor_A2SP
+++ b/projects/ithor_A2SP
@@ -1 +1 @@
-Subproject commit bdf075328c22f19bc9f57f68ec093df2b523b4d2
+Subproject commit bdf075328c22f19bc9f57f68ec093df2b523b4d2-dirty
